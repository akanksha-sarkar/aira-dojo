import os
from pathlib import Path
from typing import Any, Dict, Optional, Tuple

from dojo.core.tasks.base import Task
from dojo.config_dataclasses.task.sciduc import SciDUCTaskConfig
from dojo.utils.logger import get_logger
from dojo.tasks.sciduc.evaluate import evaluate_program
from sciduc.registry import registry

class SciDUCTask(Task):
    def __init__(self, cfg: SciDUCTaskConfig):
        super().__init__(cfg)
        self.cfg = cfg
        self.logger = get_logger()
        self.task_src_path = Path(__file__).resolve().parent
        # Get instructions.
        self.instructions_path = self.task_src_path / "instructions.txt"
        self.instructions = self.instructions_path.read_text()
        self.instructions = os.path.expandvars(self.instructions)
        # Read task description.
        task_description_path = Path(self.cfg.public_dir).resolve() / "description.md"
        self.task_description = self.instructions + "\n" + task_description_path.read_text()

        # Load competition data
        new_registry = registry.set_data_dir(Path(self.cfg.cache_dir))
        self.task = new_registry.get_task(self.cfg.name)

        self.public_dir = Path(self.cfg.public_dir).resolve()
        self.private_dir = Path(self.cfg.private_dir).resolve()

    def prepare(self, **task_args: Optional[Dict]) -> Dict:
        pass

    def step_task(self, state: Dict, action: Any) -> Tuple[Dict, Dict]:
        try:
            solution = extract_code(action)
        except Exception as e:
            self.logger.error(f"The solution does not follow the required format: {e}")
            exec_output = ExecutionResult.get_empty()
            exec_output.term_out[0] = f"Invalid solution: {e}"
            return state, {EXECUTION_OUTPUT: exec_output, VALIDATION_FITNESS: None, VALID_SOLUTION: False}
        
        interpreter = state["solver_interpreter"]
        exec_output: ExecutionResult = interpreter.run(solution, file_name=self._solution_script)
        eval_result = {EXECUTION_OUTPUT: exec_output}

        if (not exec_output.exit_code == 0) or exec_output.timed_out:
            self.logger.error(f"Execution failed - exit code: {exec_output.exit_code} - timed out: {exec_output.timed_out} - execution time: {exec_output.exec_time}")
            
            # If the execution was not succesful, for sanity reasons, we want to remove the submission file if it exists
            # this ensures that the agent does not have access to a submission file that was not generated by a successful execution
            if self._submission_file_path.exists():
                self._submission_file_path.unlink(missing_ok=True)
        else:
            self.logger.info(f"Execution successful - fetching submission file for evaluation.")
            interpreter.fetch_file(self._evaluation_file_path)
            self.logger.info(f"Submission file fetched: {self._evaluation_file_path}")
        submission_exists = self._evaluation_file_path.exists()
        eval_result[VALID_SOLUTION] = False
        if submission_exists:
            is_valid_submission, message = validate_submission(self._evaluation_file_path, self.task)
            eval_result[VALID_SOLUTION] = is_valid_submission
            eval_result[VALID_SOLUTION_FEEDBACK] = message
            self.logger.info(f"Submission file found: {self._evaluation_file_path} || Submission valid: {is_valid_submission}")

            

    def evaluate_fitness(self, solution: Optional[Dict] = None, state: Optional[Dict] = None, interpreter: Optional[Dict] = None, aux_info: Dict[str, Any] = None) -> Any:
        pass

    def close(self, state: Dict) -> None:
        pass