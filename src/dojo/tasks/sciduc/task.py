import os
from pathlib import Path
from typing import Any, Dict, Optional, Tuple

from dojo.core.tasks.base import Task
from dojo.config_dataclasses.task.sciduc import SciDUCTaskConfig
from dojo.utils.logger import get_logger
from dojo.tasks.sciduc.evaluate import evaluate_program
from sciduc.registry import registry

class SciDUCTask(Task):
    def __init__(self, cfg: SciDUCTaskConfig):
        super().__init__(cfg)
        self.cfg = cfg
        self.logger = get_logger()
        self.task_src_path = Path(__file__).resolve().parent
        # Get instructions.
        self.instructions_path = self.task_src_path / "instructions.txt"
        self.instructions = self.instructions_path.read_text()
        self.instructions = os.path.expandvars(self.instructions)
        # Read task description.
        task_description_path = Path(self.cfg.public_dir).resolve() / "description.md"
        self.task_description = self.instructions + "\n" + task_description_path.read_text()

        self.task_data_path = "/....." # HARDCODE for now

    def prepare(self, **task_args: Optional[Dict]) -> Dict:
        state = task_args
        state["init_obs"] = {}
        self._submission_file_path = Path(task_args["solver_interpreter"].working_dir) / "results.json"
        return state, {TASK_DESCRIPTION: self.task_description}

    def step_task(self, state: Dict, action: Any) -> Tuple[Dict, Dict]:
        try:
            solution = extract_code(action)
        except Exception as e:
            self.logger.error(f"The solution does not follow the required format: {e}")
            exec_output = ExecutionResult.get_empty()
            exec_output.term_out[0] = f"Invalid solution: {e}"
            return state, {EXECUTION_OUTPUT: exec_output, VALIDATION_FITNESS: None, VALID_SOLUTION: False}
        
        interpreter = state["solver_interpreter"]
        exec_output: ExecutionResult = interpreter.run(solution, file_name=self._solution_script)
        eval_result = {EXECUTION_OUTPUT: exec_output}

        if (not exec_output.exit_code == 0) or exec_output.timed_out:
            self.logger.error(f"Execution failed - exit code: {exec_output.exit_code} - timed out: {exec_output.timed_out} - execution time: {exec_output.exec_time}")
            
            # If the execution was not succesful, for sanity reasons, we want to remove the submission file if it exists
            # this ensures that the agent does not have access to a submission file that was not generated by a successful execution
            if self._submission_file_path.exists():
                self._submission_file_path.unlink(missing_ok=True)
        else:
            self.logger.info(f"Execution successful - fetching submission file for evaluation.")
            interpreter.fetch_file(self._submission_file_path)
            self.logger.info(f"Submission file fetched: {self._submission_file_path}")
        submission_exists = self._submission_file_path.exists()
        eval_result[VALID_SOLUTION] = False
        if submission_exists:
            is_valid_submission, message = validate_submission(self._submission_file_path, self.task_data_path)
            eval_result[VALID_SOLUTION] = is_valid_submission
            eval_result[VALID_SOLUTION_FEEDBACK] = message
            self.logger.info(f"Submission file found: {self._submission_file_path} || Submission valid: {is_valid_submission}")
            if is_valid_submission:
                metrics = grade(self._submission_file_path, self.task_data_path)
                eval_result[TEST_FITNESS] = metrics["AP@0.5"]
                self.logger.info(f"Test fitness: {eval_result[TEST_FITNESS]}")
            self._submission_file_path.unlink(missing_ok=True) # remove the submission_file locally
            assert not self._submission_file_path.exists(), (
                "At this point, the submissions file should not exists locally!"
            )
        if interpreter.factory:
            interpreter.close()
        else:
            interpreter.run(
                f"!rm {self.cfg.submission_fname}"
            )  # remove the submission_file from the agent's environment
            assert interpreter.fetch_file(self._submission_file_path) is None, (
                "At this point, the submissions file should not exists in the agent's environment!"
            )

        return state, eval_result

    def evaluate_fitness(self, solution: Optional[Dict] = None, state: Optional[Dict] = None, interpreter: Optional[Dict] = None, aux_info: Dict[str, Any] = None) -> Any:
        if self._submission_file_path is None:
            raise Exception("The path to the submission file must be set.")
        
        exec_output = interpreter.run(solution, file_name=self._solution_script)
        eval_result = {EXECUTION_OUTPUT: exec_output}
        
        interpreter.fetch_file(self._submission_file_path)
        has_csv_submission = self._submission_file_path.exists()
        assert has_csv_submission, "The final solution is not valid!"

        metrics = grade(self._submission_file_path, self.task_data_path)
        eval_result[TEST_FITNESS] = metrics["AP@0.5"]
        self.logger.info(f"Test fitness: {eval_result[TEST_FITNESS]}")

        return eval_result

    def close(self, state: Dict) -> None:
        for interp_key in ["solver_interpreter", "eval_interpreter"]:
            if interp_key not in state:
                continue

            interpreter = state[interp_key]
            if hasattr(interpreter, "cleanup_session"):
                interpreter.cleanup_session()

            if hasattr(interpreter, "clean_up"):
                interpreter.clean_up()

def validate_submission(submission: Path, task: Task) -> tuple[bool, str]:
    """
    Validates a submission for the given competition by actually running the competition grader.
    This is designed for end users, not developers (we assume that the competition grader is
    correctly implemented and use that for validating the submission, not the other way around).
    """
    if not submission.is_file():
        return False, f"Submission invalid! Submission file {submission} does not exist."

    if not submission.suffix.lower() == ".json":
        return False, "Submission invalid! Submission file must be a CSV file."

    if not is_dataset_prepared(task, grading_only=True):
        raise ValueError("Dataset for task is not prepared!")

    try:
        grade(submission, task.answers)
    except Exception as e:
        return (
            False,
            f"Submission invalid! The attempt to grade the submission has resulted in the following error message:\n{e}",
        )

    return True, "Submission is valid."

from pycocotools.coco import COCO
from pycocotools.cocoeval import COCOeval

def grade(submission, answer):
    """
      Answer is a path to a JSON file containing the ground truth annotations in COCO format.
      Submission is a path to a json file containing the predictions in COCO format.
    """
    coco_gt = COCO(answer)
    with open(submission, "r") as f:
        coco_predictions = json.load(f)
    coco_dt = coco_gt.loadRes(coco_predictions)
    evaluator = COCOeval(coco_gt, coco_dt, iou_type="bbox")
    evaluator.evaluate()
    evaluator.accumulate()
    evaluator.summarize()
    stats = evaluator.stats
    labels = [
        "AP@[.5:.95]",
        "AP@0.5",
        "AP@0.75",
        "AP (small)",
        "AP (medium)",
        "AP (large)",
        "AR@1",
        "AR@10",
        "AR@100",
        "AR (small)",
        "AR (medium)",
        "AR (large)"
    ]
    metrics = {}
    # print("Evaluation results:")
    for name, value in zip(labels, stats):
        # print(f"{name:<15}: {value:.4f}")
        metrics[name] = value
    metrics = {"AP@0.5": metrics["AP@0.5"]}
    return metrics