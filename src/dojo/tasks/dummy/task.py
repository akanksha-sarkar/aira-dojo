import os
from pathlib import Path
from typing import Any, Dict, Optional, Tuple

from dojo.core.tasks.base import Task
from dojo.config_dataclasses.task.dummy import DummyTaskConfig
from dojo.utils.logger import get_logger
from sciduc.registry import registry
from dojo.tasks.dummy.grade import validate_submission, grade
from dojo.core.tasks.base import ExecutionResult
from dojo.core.tasks.constants import (
    EXECUTION_OUTPUT,
    TASK_DESCRIPTION,
    TEST_FITNESS,
    VALID_SOLUTION_FEEDBACK,
    VALIDATION_FITNESS,
    AUX_EVAL_INFO,
    VALID_SOLUTION,
)
from dojo.utils.code_parsing import extract_code

class DummyTask(Task):
    def __init__(self, cfg: DummyTaskConfig):
        super().__init__(cfg)
        self.cfg = cfg
        self.logger = get_logger()
        self.task_src_path = Path(__file__).resolve().parent
        self.task_description = "This is a dummy task."
        self._solution_script = "solution.py"
        self.task_data_path = "/....." # HARDCODE for now

    def prepare(self, **task_args: Optional[Dict]) -> Dict:
        state = task_args
        state["init_obs"] = {}
        self._submission_file_path = Path(task_args["solver_interpreter"].working_dir) / self.cfg.submission_fname
        return state, {TASK_DESCRIPTION: self.task_description}

    def step_task(self, state: Dict, action: Any) -> Tuple[Dict, Dict]:
        try:
            solution = extract_code(action)
        except Exception as e:
            self.logger.error(f"The solution does not follow the required format: {e}")
            exec_output = ExecutionResult.get_empty()
            exec_output.term_out[0] = f"Invalid solution: {e}"
            return state, {EXECUTION_OUTPUT: exec_output, VALIDATION_FITNESS: None, VALID_SOLUTION: False}
        
        interpreter = state["solver_interpreter"]
        exec_output: ExecutionResult = interpreter.run(solution, file_name=self._solution_script)
        self.logger.info(f"Execution output: {exec_output}")
        eval_result = {EXECUTION_OUTPUT: exec_output}

        # Check if the execution was not successful
        if (not exec_output.exit_code == 0) or exec_output.timed_out:
            self.logger.error(f"Execution failed - exit code: {exec_output.exit_code} - timed out: {exec_output.timed_out} - execution time: {exec_output.exec_time}")
            
            # If the execution was not succesful, for sanity reasons, we want to remove the submission file if it exists
            # this ensures that the agent does not have access to a submission file that was not generated by a successful execution
            if self._submission_file_path.exists():
                self._submission_file_path.unlink(missing_ok=True)
        else:
            self.logger.info(f"Execution successful - fetching submission file for evaluation.")
            interpreter.fetch_file(self._submission_file_path)
            self.logger.info(f"Submission file fetched: {self._submission_file_path}")

        # Check if the submission file exists
        submission_exists = self._submission_file_path.exists()
        eval_result[VALID_SOLUTION] = False
        if submission_exists:
            is_valid_submission, message = validate_submission(self._submission_file_path, self.task_data_path)
            eval_result[VALID_SOLUTION] = is_valid_submission
            eval_result[VALID_SOLUTION_FEEDBACK] = message
            self.logger.info(f"Submission file found: {self._submission_file_path} || Submission valid: {is_valid_submission}")
            if is_valid_submission:
                # Grade the submission
                metrics = grade(self._submission_file_path, self.task_data_path)
                eval_result[TEST_FITNESS] = metrics["AP@0.5"]
                self.logger.info(f"Test fitness: {eval_result[TEST_FITNESS]}")
            self._submission_file_path.unlink(missing_ok=True) # remove the submission_file locally
            assert not self._submission_file_path.exists(), (
                "At this point, the submissions file should not exists locally!"
            )
        # Close the interpreter
        if interpreter.factory:
            interpreter.close()
        else:
            # Remove the submission_file from the agent's environment
            interpreter.run(
                f"!rm {self.cfg.submission_fname}"
            )  # remove the submission_file from the agent's environment

            assert interpreter.fetch_file(self._submission_file_path) is None, (
                "At this point, the submissions file should not exists in the agent's environment!"
            )

        return state, eval_result

    def evaluate_fitness(self, solution: Optional[Dict] = None, state: Optional[Dict] = None, interpreter: Optional[Dict] = None, aux_info: Dict[str, Any] = None) -> Any:
        if self._submission_file_path is None:
            raise Exception("The path to the submission file must be set.")
        
        exec_output = interpreter.run(solution, file_name=self._solution_script)
        self.logger.info(f"Execution output: {exec_output}")
        eval_result = {EXECUTION_OUTPUT: exec_output}
        
        interpreter.fetch_file(self._submission_file_path)
        has_csv_submission = self._submission_file_path.exists()
        assert has_csv_submission, "The final solution is not valid!"

        metrics = grade(self._submission_file_path, self.task_data_path)
        eval_result[TEST_FITNESS] = metrics["AP@0.5"]
        self.logger.info(f"Test fitness: {eval_result[TEST_FITNESS]}")

        return eval_result

    def close(self, state: Dict) -> None:
        for interp_key in ["solver_interpreter", "eval_interpreter"]:
            if interp_key not in state:
                continue

            interpreter = state[interp_key]
            if hasattr(interpreter, "cleanup_session"):
                interpreter.cleanup_session()

            if hasattr(interpreter, "clean_up"):
                interpreter.clean_up()

